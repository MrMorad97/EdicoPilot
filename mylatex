% !TEX TS-program = pdflatex
\documentclass[a4paper,12pt]{article}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage{graphicx}
\usepackage[table,xcdraw]{xcolor}
\usepackage{float}
\usepackage{geometry}
\usepackage{fancyhdr}
\usepackage{titlesec}
\usepackage{subcaption}
\usepackage{hyperref}
\usepackage{listings}
\usepackage{lipsum}
\usepackage{longtable}
\usepackage{tabularx}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{array}
\usepackage{amsmath}
\usepackage{tcolorbox}
\usepackage{algorithm2e}
\usepackage{tikz}
\usepackage[most]{tcolorbox}
\usepackage{enumitem}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage[most]{tcolorbox}
\tcbuselibrary{breakable}

\newtcolorbox{pseudocodebox}[1][]{
  colback=gray!10!white,
  colframe=blue!80!black,
  boxrule=1pt,
  arc=4pt,
  left=6pt,
  right=6pt,
  top=6pt,
  bottom=6pt,
  fonttitle=\bfseries,
  fontupper=\small\ttfamily,
  title=Pseudocode,
  breakable=true,
  width=\linewidth,
  parbox=false,
  breaklines=true,
  wrap=true,
  breakanywhere=true,
  before=\vspace{0.5em},
  after=\vspace{0.5em},
  #1
}

% For better spacing in table rows
\renewcommand{\arraystretch}{1.5}

% Adjusting margins
\geometry{
    top=2.5cm,
    bottom=2.5cm,
    left=2.5cm,
    right=2.5cm
}

% Modern color definitions
\definecolor{maincolor}{RGB}{64, 64, 64}
\definecolor{modernblue}{RGB}{70, 130, 180}
\definecolor{moderngray}{RGB}{231, 139, 72}
\definecolor{lightgray}{RGB}{22, 153, 118}
\definecolor{lightblue}{RGB}{173, 216, 230}

% Customizing section titles
\titleformat{\section}
{\normalfont\Large\bfseries\color{modernblue}}
{\thesection}{1em}{}
\titleformat{\subsection}
{\normalfont\large\bfseries\color{moderngray}}
{\thesubsection}{1em}{}
\titleformat{\subsubsection}
{\normalfont\normalsize\bfseries\color{lightgray}}
{\thesubsubsection}{1em}{}

% Hyperlink styling
\hypersetup{
    colorlinks=true,
    linkcolor=modernblue,
    urlcolor=modernblue,
    citecolor=modernblue,
    pdfborder={0 0 0}
}

\renewcommand{\baselinestretch}{1.2}

% Header and footer
\pagestyle{fancy}
\fancyhf{}
\fancyhead[L]{Hammad-Boussagman}
\fancyhead[R]{Multi-Model NLP Project}
\fancyfoot[C]{\thepage}

% Informations sur l'Ã©tudiant
\newcommand{\studentname}{\Large Your Name}
\newcommand{\coursename}{Multi-Model NLP Project}
\newcommand{\masterprogram}{\Large Master in Artificial Intelligence}

% Define a custom style for code listings
\lstdefinestyle{mystyle}{
    backgroundcolor=\color{lightgray!10},
    commentstyle=\color{codegreen},
    keywordstyle=\color{magenta},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{codepurple},
    basicstyle=\ttfamily\footnotesize,
    breakatwhitespace=false,
    breaklines=true,
    captionpos=b,
    keepspaces=true,
    numbers=left,
    numbersep=5pt,
    showspaces=false,
    showstringspaces=false,
    showtabs=false,
    tabsize=4,
    frame=single,
    rulecolor=\color{black},
    frameround=tttt
}
\lstset{style=mystyle}
\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}

\begin{document}

\begin{titlepage}
    \centering
    \vspace*{0.5cm}
    \includegraphics[width=0.7\textwidth]{fslogo.png}\\[1cm]
    
    {\Huge\bfseries\color{modernblue} Advancing Conversational AI: Training, Fine-Tuning, and Retrieval-Augmented Generation \par}
    \vspace{0.3cm}
    {\LARGE Lightweight Language Model (LLM) \par}
    \vspace{1.5cm}
    
    % Student profiles with circular images
    \begin{center}
        \begin{tabular}{c@{\hspace{3cm}}c}
            % First row: images
            \begin{tikzpicture}
                \begin{scope}
                    \clip (0,0) circle (1.5cm);
                    \node[anchor=center] at (0,0) {
                        \includegraphics[width=3cm, height=3cm, keepaspectratio]{ayoub_photo.jpg}
                    };
                \end{scope}
                \draw[modernblue, line width=2pt] (0,0) circle (1.5cm);
            \end{tikzpicture}
            &
            \begin{tikzpicture}
                \begin{scope}
                    \clip (0,0) circle (1.5cm);
                    \node[anchor=center] at (0,0) {
                        \includegraphics[width=3cm, height=3cm, keepaspectratio]{morad_photo.jpg}
                    };
                \end{scope}
                \draw[modernblue, line width=2pt] (0,0) circle (1.5cm);
            \end{tikzpicture}
            \\
            % Second row: names
            \textbf{\Large Ayoub Hammad}
            &
            \textbf{\Large Morad Boussagman}
        \end{tabular}
    \end{center}
    
    \vspace{1cm}
    
    % Project details in a professional box
    \begin{tcolorbox}[
        colback=lightblue!10,
        colframe=modernblue,
        boxrule=2pt,
        arc=10pt,
        width=0.75\textwidth,
        center
    ]
    \centering
    \textbf{\Large Program:} Embedded Artificial Intelligence \\
    \vspace{0.2cm}
    \textbf{\Large Instructor:} Pr. El Mehdi Cherrat \\
    \vspace{0.2cm}
    \textbf{\Large Academic Year:} 2023-2024
    \end{tcolorbox}
    
    \vspace{0.5cm}
    \textbf{\normalsize \today}
\end{titlepage}

\tableofcontents
\newpage

% ===================== INTRODUCTION =====================
\section{Introduction}

This project presents an innovative approach to Natural Language Processing (NLP) by combining multiple deep learning models to create an advanced conversational chatbot system. The main objective is to develop a multi-model platform capable of handling different types of linguistic interactions with maximum precision and flexibility.

The developed system integrates four complementary approaches:
\begin{itemize}[label=--]
    \item \textbf{Custom Transformer Model:} A deep learning model trained from scratch on DailyDialog Corpus .
    \item \textbf{RAG (Retrieval-Augmented Generation) System:} Using TinyLlama with ChromaDB for document-based responses .
    \item \textbf{Fine-tuned GPT-2:} Adaptation of a pre-trained model to improve its conversational capabilities ( using the same DailyDialog corpus) .
    \item \textbf{Streamlit Interface:} Modern Streamlit web interface to interact with all models .
\end{itemize}

\noindent
This multi-model approach allows us to benefit from the advantages of each technique while minimizing their respective limitations. The user interface developed with Streamlit offers an intuitive user experience to interact with all models.

% ===================== GENERAL CONTEXT AND PROBLEMATIC =====================
\section{General Context and Problematic}

\subsection{Deep Learning Context in NLP}

Natural Language Processing has experienced a revolution with the advent of models based on the Transformer architecture. These models, introduced by Vaswani et al. in 2017, have enabled significant advances in text understanding and generation.

Current challenges in NLP include:
\begin{itemize}[label=--]
    \item The need for specialized models for specific domains
    \item The importance of factual accuracy in responses
    \item Managing language model hallucination
    \item Adapting to new information without complete retraining
\end{itemize}

\subsection{Identified Problematic}

The main problematic of this project is to develop a system capable of:
\begin{enumerate}[label=--]
    \item Generating natural and contextually appropriate conversational responses
    \item Providing accurate information based on external documents
    \item Adapting models to specific domains without loss of general performance
    \item Offering an intuitive user interface for different types of interactions
\end{enumerate}
\noindent
This problematic requires a hybrid approach combining several deep learning techniques to optimize each aspect of the system.

\subsection{Constraints and Challenges Encountered}

\subsubsection{Hardware Constraints}

One of the major challenges encountered during the development of this project was the limitation of available hardware resources:

\begin{itemize}[label=--]
    \item \textbf{Google Colab Limitations:} Free Colab sessions impose strict constraints:
    \begin{itemize}
        \item Session time limited to 4 hours maximum
        \item Limited GPU memory (Tesla T4 with 16GB VRAM)
        \item Risk of automatic disconnection after inactivity
        \item No data persistence between sessions
    \end{itemize}
    
    \item \textbf{Impact on Training:} These limitations required adaptations:
    \begin{itemize}
        \item Frequent saving of models and checkpoints
        \item Optimization of model size to fit in memory
        \item Strategy for resuming training in case of disconnection
        \item Use of compression techniques for embeddings
    \end{itemize}
\end{itemize}

\subsubsection{Logical and Technical Problems}

\paragraph{Custom Model Convergence Problem}
Training the custom transformer model revealed specific challenges:

\begin{itemize}[label=--]
    \item \textbf{Early overfitting:} The model tended to memorize training data
    \item \textbf{Gradient instability:} Need to adjust learning rate and use gradient clipping
    \item \textbf{Vocabulary limitation:} The custom vocabulary limited response diversity
\end{itemize}

\paragraph{RAG System Complexity}
Implementing the RAG system presented several challenges:

\begin{itemize}[label=--]
    \item \textbf{Retrieval latency:} Search time in the vector database impacted user experience .
    \item \textbf{Embedding quality:} The choice of embedding model significantly affected retrieval relevance .
    \item \textbf{Memory management:} Storing embeddings and cache consumed a lot of RAM .
    \item \textbf{Streaming Response:} Implementing Streaming mechanism rather then waiting until full response generation .
\end{itemize}

\paragraph{Fine-tuning Optimization}
GPT-2 fine-tuning encountered specific obstacles:

\begin{itemize}[label=--]
    \item \textbf{Model size:} GPT-2 required a lot of GPU memory
    \item \textbf{Tokenizer adaptation:} Management of special tokens and padding
    \item \textbf{Adaptation/specialization balance:} Avoid losing general capabilities
    \item \textbf{Training time:} Long duration necessary for effective fine-tuning
\end{itemize}

\subsubsection{Adopted Solutions}

To overcome these challenges, several strategies were implemented:

\begin{enumerate}
    \item \textbf{Memory optimization:}
    \begin{itemize}[label=--]
        \item Reduction of custom model size (192 dimensions instead of 512)
        \item Use of gradient accumulation techniques
        \item Implementation of an intelligent cache system for RAG
    \end{itemize}
    
    \item \textbf{Robust backup strategy:}
    \begin{itemize}[label=--]
        \item Automatic backup every 5 epochs
        \item Storage of checkpoints on Google Drive
        \item Automatic training resumption system
    \end{itemize}
    
    \item \textbf{Performance optimization:}
    \begin{itemize}[label=--]
        \item Use of FastEmbed for fast embeddings
        \item Implementation of streaming for the user interface
        \item Cache of RAG responses to avoid recalculations
    \end{itemize}
\end{enumerate}

% ===================== PROJECT DESCRIPTION =====================
\section{Project Description}

\subsection{System Overview}

The EduCopilot project is a multi-model conversational chatbot system that integrates four main components:

\begin{enumerate}
    \item \textbf{Custom Transformer Model:} A deep learning model developed from scratch, specially designed for conversations and trained on the DailyDialog dataset.
    
    \item \textbf{RAG (Retrieval-Augmented Generation) System:} A retrieval-augmented generation system using TinyLlama, ChromaDB and LangChain to provide responses based on external documents.
    
    \item \textbf{Fine-tuned GPT-2:} Adaptation of the pre-trained GPT-2 model to improve its performance on specific conversational tasks.
    
    \item \textbf{Streamlit Interface:} A modern web interface allowing users to interact with all models and upload PDF documents.
\end{enumerate}
\noindent
This multi-model architecture allows us to benefit from the advantages of each approach: specialization for conversations (custom model), factual accuracy (RAG), domain adaptation (fine-tuning), and accessibility (user interface).

\subsection{System Components}

\subsubsection{Custom Transformer Model}
A deep learning model developed from scratch using the Transformer architecture. This model is specially designed for conversations and trained on the DailyDialog dataset.

\subsubsection{RAG System with TinyLlama}
A retrieval-augmented generation system using:
\begin{itemize}[label=--]
    \item TinyLlama as the generation model
    \item ChromaDB for vector storage
    \item FastEmbed for embeddings
    \item LangChain for orchestration
\end{itemize}

\subsubsection{Fine-tuned GPT-2}
Adaptation of the pre-trained GPT-2 model to improve its performance on specific conversational tasks . Using the same dataset we trained our model on .

\subsubsection{Streamlit Interface}
A modern web interface allowing users to interact with all models and upload PDF documents.

\begin{figure}[H]
    \centering
    \fcolorbox{black}{white}{\includegraphics[width=0.8\textwidth]{multimodel_chatbot_modes.png}}
    \caption{Model selection dropdown menu in the EduCopilot multi-model interface.}
\end{figure}

\begin{figure}[H]
    \centering
    \fcolorbox{black}{white}{\includegraphics[width=0.8\textwidth]{custom_lm_ui.png}}
    \caption{Chat interface in Custom LM mode.}
\end{figure}

\begin{figure}[H]
    \centering
    \fcolorbox{black}{white}{\includegraphics[width=0.8\textwidth]{gpt2_ui.png}}
    \caption{Chat interface in GPT-2 mode.}
\end{figure}
\begin{figure}[H]
    \centering
    \fcolorbox{black}{white}{\includegraphics[width=0.8\textwidth]{finetuned_gpt2_ui.png}}
    \caption{Chat interface in Fine-Tuned GPT-2 mode.}
\end{figure}

\begin{figure}[H]
    \centering
    \fcolorbox{black}{white}{\includegraphics[width=0.8\textwidth]{tinyllama_no_pdf_ui.png}}
    \caption{Chat interface in TinyLlama mode (without PDF).}
\end{figure}
\begin{figure}[H]
    \centering
    \fcolorbox{black}{white}{\includegraphics[width=0.8\textwidth]{tinyllama_with_pdf_ui.png}}
    \caption{Chat interface in TinyLlama mode (with PDF).}
\end{figure}





\subsection{Technologies Used}

\begin{table}[H]
    \centering
    \begin{tabular}{|l|l|}
        \hline
        \textbf{Technology} & \textbf{Usage} \\
        \hline
        PyTorch & Deep learning framework for custom transformer model \\
        Transformers & Pre-trained models (GPT-2, BERT) and tokenizers \\
        LangChain & RAG orchestration and document processing \\
        LangChain-Community & Vector stores and embeddings integration \\
        LangChain-Core & Core components and prompt templates \\
        ChromaDB & Vector database for document storage \\
        Streamlit & Web interface and user interaction \\
        Ollama & Local model server for TinyLlama \\
        FastEmbed & High-speed embedding model (BAAI/bge-small-en-v1.5) \\
        PyPDF2/PyPDF & PDF document processing and text extraction \\
        BERTScore & Text generation evaluation metrics \\
        Matplotlib & Data visualization and plotting \\
        NumPy & Numerical computations and array operations \\
        Pickle & Model serialization and caching \\
        Hashlib & Response caching and deduplication \\
        Re & Text preprocessing and pattern matching \\
        Typing & Type hints and annotations \\
        Collections & Data structures (deque for conversation context) \\
        Functools & Caching decorators (lru\_cache) \\
        \hline
    \end{tabular}
    \caption{Comprehensive project technology stack}
\end{table}

% ===================== DEEP LEARNING MODEL ARCHITECTURE =====================
\newpage
\section{Deep Learning Model Architecture Used}

\subsection{Transformer Architecture}

The core of the system relies on the Transformer architecture, which uses the attention mechanism to process text sequences.

\begin{figure}[H]
    \centering
    \fcolorbox{black}{white}{\includegraphics[width=0.9\textwidth]{What_is_a_Transformer.png}}
    \caption{The transformer architecture as a black box.}
\end{figure}

\subsubsection{Attention Mechanism}

Attention allows the model to focus on different parts of the input according to context:

\begin{pseudocodebox}
\textbf{Attention(Q, K, V) = softmax($\frac{QK^T}{\sqrt{d_k}}$)V}
\end{pseudocodebox}

Where:
\begin{itemize}
    \item Q : Query matrix
    \item K : Key matrix  
    \item V : Value matrix
    \item $d_k$ : Key dimension
\end{itemize}

\subsubsection{Multi-Head Attention Architecture}

The model uses multi-head attention to capture different types of relationships:

\begin{pseudocodebox}
\textbf{MultiHeadAttention}($Q, K, V$): \\
\> \textbf{For } $i = 1$ \textbf{ to } $h$: \\
\> \> $\text{head}_i \gets \text{Attention}(Q W_i^Q,\ K W_i^K,\ V W_i^V)$ \\
\> $H \gets \text{Concat}(\text{head}_1, \dots, \text{head}_h)$ \\
\> \textbf{Return } $H W^O$
\end{pseudocodebox}


\subsection{Custom Transformer Model}

\subsubsection{Detailed Architecture}

The custom model implements a decoder-only architecture with the following parameters:

\begin{table}[H]
    \centering
    \begin{tabular}{|l|c|}
        \hline
        \textbf{Parameter} & \textbf{Value} \\
        \hline
        Model dimension ($d_{model}$) & 192 \\
        Number of attention heads & 8 \\
        Number of layers & 4 \\
        Feed-forward dimension & 512 \\
        Maximum length & 128 \\
        Dropout & 0.1 \\
        \hline
    \end{tabular}
    \caption{Custom model hyperparameters}
\end{table}

\subsubsection{Model Components}
\begin{pseudocodebox}
\textbf{Class LightweightTransformerChatbot}:
    \begin{itemize}
        \item \textbf{Embedding Layer}: Token conversion $\rightarrow$ $d_{\text{model}}$ vectors
        \item \textbf{Positional Encoding}: Addition of positional information
        \item \textbf{Transformer Blocks}: 
            \begin{itemize}
                \item 4 stacked blocks
                \item Multi-head attention (8 heads)
                \item Feed-forward (dimension $d_{\text{ff}} = 2048$)
                \item Layer normalization
            \end{itemize}
        \item \textbf{Output Projection}: Projection to vocabulary space $V$
    \end{itemize}
\end{pseudocodebox}

\subsection{RAG System}

\subsubsection{RAG Architecture}

The RAG system combines information retrieval and generation:

\begin{pseudocodebox}
\textbf{RAG Pipeline}:
    \begin{enumerate}
        \item \textbf{PDF Ingestion}: Loading and cleaning documents
        \item \textbf{Text Splitting}: Splitting into chunks (800 characters)
        \item \textbf{Embedding}: Conversion to vectors with FastEmbed
        \item \textbf{Vector Store}: Storage in ChromaDB
        \item \textbf{Retrieval}: Search for the 3 most relevant chunks
        \item \textbf{Generation}: Generation with TinyLlama (7B parameters)
        \item \textbf{Caching}: Response caching (TTL=24h)
    \end{enumerate}
\end{pseudocodebox}
\begin{figure}[H]
    \centering
    \fcolorbox{black}{white}{\includegraphics[width=0.9\textwidth]{rag.png}}
    \caption{RAG pipeline}
\end{figure}


\subsubsection{RAG Components}

\begin{itemize}[label=--]
    \item \textbf{Text Splitter:} RecursiveCharacterTextSplitter with 150 character overlap
    \item \textbf{Embeddings:} BAAI/bge-small-en-v1.5 for quality and speed
    \item \textbf{Retriever:} MMR (Maximal Marginal Relevance) with k=4
    \item \textbf{LLM:} TinyLlama via Ollama with temperature 0.7
\end{itemize}

\subsection{Fine-tuning GPT-2}

\begin{pseudocodebox}
\textbf{Fine-tuning Loop}:
    \textbf{For each epoch}:
        \begin{itemize}
            \item \textbf{For each batch}:
                \begin{itemize}
                    \item Calculate predictions (forward pass)
                    \item Calculate cross-entropy loss
                    \item Backpropagation (backward pass)
                    \item Update weights (optimizer.step())
                \end{itemize}
            \item Evaluation on validation set
            \item Save model if improvement
        \end{itemize}
\end{pseudocodebox}



% ===================== FUNCTIONING AND IMPLEMENTATION STEPS =====================
\section{Functioning and Implementation Steps}

\subsection{Step 1: Data Preparation}

\subsubsection{DailyDialog Dataset}

The project uses the DailyDialog dataset, containing multi-turn conversations on various topics:

\begin{table}[H]
\centering
\begin{minipage}{0.45\textwidth}
    \centering
    \begin{tabular}{|l|c|}
        \hline
        \textbf{Set} & \textbf{Number of dialogues} \\
        \hline
        Training & 76,053 \\
        Validation & 7,069 \\
        Test & 6,740 \\
        \hline
    \end{tabular}
    \caption{Dataset distribution}
    \label{tab:dailydialog}
\end{minipage}
\hfill
\begin{minipage}{0.45\textwidth}
    \centering
    \includegraphics[width=\linewidth]{sample.png} % Replace with your image file
    \captionof{figure}{Data sample}
    \label{fig:dailydialog-example}
\end{minipage}
\end{table}


\subsubsection{Preprocessing}

\begin{pseudocodebox}
\textbf{Data preprocessing}:
    \begin{enumerate}
        \item \textbf{Cleaning}:
            \begin{itemize}
                \item Removal of special characters
                \item Unicode normalization
                \item Correction of whitespace
            \end{itemize}
        \item \textbf{Tokenization}:
            \begin{itemize}
                \item Splitting with custom vocabulary
                \item Handling of unknown words (UNK)
                \item Addition of special tokens (CLS, SEP)
            \end{itemize}
        \item \textbf{Padding/Truncation}:
            \begin{itemize}
                \item Alignment to fixed length $L_{max}=512$
                \item Padding masking
            \end{itemize}
        \item \textbf{Batching}:
            \begin{itemize}
                \item Grouping by size $B=32$
                \item Random shuffling
            \end{itemize}
    \end{enumerate}
\end{pseudocodebox}

\subsection{Step 2: Custom Model Training}

\subsubsection{Training Configuration}

\begin{table}[H]
    \centering
    \begin{tabular}{|l|c|}
        \hline
        \textbf{Parameter} & \textbf{Value} \\
        \hline
        Optimizer & Adam \\
        Learning Rate & 0.001 \\
        Batch Size & 32 \\
        Epochs & 30 \\
        Device & CUDA (Tesla T4) \\
        \hline
    \end{tabular}
    \caption{Training configuration}
\end{table}

\subsubsection{Training Monitoring}

According to training logs, the model converged efficiently:

\begin{itemize}
    \item Epoch 1 : Loss = 4.3154 â 3.7795 (validation)
    \item Epoch 6 : Loss = 2.8023 â 3.3457 (validation)
    \item Epoch 30 : Loss = 2.0905 â 3.4347 (validation)
\end{itemize}

\subsection{Step 3: RAG System Implementation}

\subsubsection{RAG Configuration}

\begin{pseudocodebox}
\textbf{RAG Configuration}:

    \begin{tabular}{ll}
        \textbf{Chunk Size}       & : 800 characters \\
        \textbf{Chunk Overlap}    & : 150 characters \\
        \textbf{Top-k Retrieval}  & : 4 documents \\
        \textbf{Score Threshold}  & : 0.4 \\
        \textbf{Context Limit}    & : 2500 characters \\
        \textbf{Embedding Model}  & : FastEmbed \\
        \textbf{Vector Store}     & : ChromaDB \\
        \textbf{LLM}             & : TinyLlama-7B \\
    \end{tabular}
\end{pseudocodebox}
\subsubsection{RAG Optimizations}

\begin{itemize}[label=--]
    \item \textbf{Caching} : Response caching to avoid recalculations
    \item \textbf{Streaming} : Real-time generation for UX
    \item \textbf{Error Handling} : Robust error handling
    \item \textbf{Memory Management} : Cache limitation to 50 entries
\end{itemize}

\newpage

\subsection{Step 4: GPT-2 Fine-tuning}
\begin{pseudocodebox}
\textbf{Fine-tuning Steps}:
    \begin{enumerate}
        \item \textbf{Model loading}:
              \begin{itemize}
                  \item GPT-2 base (124M parameters)
                  \item Pre-trained weights
              \end{itemize}
        \item \textbf{Adaptation}:
              \begin{itemize}
                  \item Specialized tokenizer configuration
                  \param{Hyperparameter adjustment}
                  \item Learning rate: 5e-5
              \end{itemize}
        \item \textbf{Training}:
              \begin{itemize}
                  \item Data: DailyDialog (13k dialogues)
                  \item Batch size: 32
                  \item Epochs: 10
              \end{itemize}
        \item \textbf{Saving}:
              \begin{itemize}
                  \item Final model
                  \item Adapted tokenizer
                  \item Evaluation metrics
              \end{itemize}
    \end{enumerate}
\end{pseudocodebox}

\subsection{Step 5: Interface Development}
\begin{pseudocodebox}
\vspace{0.5em}
\begin{tabularx}{\linewidth}{@{} l X @{}}
\textbf{Sidebar}           & Model selection (LLM / PDF) and document upload \\
\textbf{Chat Area}         & Scrollable message history with Markdown formatting \\
\textbf{Input Area}        & Text input field with send and reset buttons \\
\textbf{Status Indicators} & Visual indicators for connection, loading, or errors \\
\textbf{Model Controls}    & Adjustable parameters: Prompt \\
\end{tabularx}
\end{pseudocodebox}

\newpage
\subsubsection{Interface Features}
\begin{itemize}[label=--]
    \item \textbf{Multi-models} : Switching between 4 modes
    \item \textbf{PDF Upload} : Multi-file support
    \item \textbf{Streaming} : Real-time responses
    \item \textbf{Cache Management} : Automatic cache management
\end{itemize}

% ===================== RESULTS ANALYSIS =====================
% ===================== RESULTS ANALYSIS =====================
\section{Results Analysis}

\subsection{Quantitative Evaluation}

\subsubsection{BERTScore Metrics}
\begin{figure}[H]
    \centering
    \fcolorbox{black}{white}{\includegraphics[width=0.9\textwidth]{bert_score_plot.png}}
    \caption{BERTScore F1 per turn for dialogue models}
\end{figure}

The evaluation uses BERTScore F1 to compare model performances:

\begin{table}[H]
    \centering
    \begin{tabular}{|l|c|c|c|}
        \hline
        \textbf{Model} & \textbf{Custom LM} & \textbf{GPT-2 Base} & \textbf{Fine-tuned GPT-2} \\
        \hline
        BERTScore F1 & 0.84 & 0.82 & 0.86 \\
        \hline
    \end{tabular}
    \caption{BERTScore F1 comparison (approximate results)}
\end{table}
\newpage
\subsubsection{Performance Analysis}
\begin{itemize}[label=--]
    \item \textbf{Fine-tuned GPT-2} : Best performance (+4.86\%)
    \item \textbf{Custom LM} : Strong baseline (+2.44\%)
    \item \textbf{GPT-2 Base} : Reference score
\end{itemize}
\noindent
\textbf{Note:} These metrics are based on sampled test dialogues. Results may vary depending on context and complexity.


% ------------------ TRAINING CURVES ------------------
\subsection{Training Curves Analysis}

\subsubsection{Custom Model}

\begin{figure}[H]
    \centering
    \fcolorbox{black}{white}{\includegraphics[width=0.7\textwidth]{training_and_validation_loss_over_epochs.png}}
    \caption{Training and validation loss over epochs (Custom LM)}
\end{figure}

\subsubsection{Fine-tuned GPT-2}

\begin{figure}[H]
    \centering
    \fcolorbox{black}{white}{\includegraphics[width=0.7\textwidth]{gpt2_fine_tuning_loss_per_epoch.png}}
    \caption{GPT-2 fine-tuning loss per epoch}
\end{figure}

\textbf{Observations:}
\begin{itemize}[label=--]
    \item Stable convergence
    \item No overfitting observed
\end{itemize}

% ------------------ RAG SYSTEM ------------------
\subsection{RAG System Performance}

\subsubsection{Retrieval Metrics}

\begin{table}[H]
    \centering
    \begin{tabular}{|l|c|}
        \hline
        \textbf{Metric} & \textbf{Value} \\
        \hline
        Average retrieval time & 30s \\
        Retrieved chunks precision & 85\% \\
        Context coverage & 92\% \\
        \hline
    \end{tabular}
    \caption{RAG system retrieval performance}
\end{table}

% ------------------ MODEL COMPARISON ------------------
\subsection{Multi-model Comparison}

\subsubsection{Advantages of Each Approach}

\begin{table}[H]
    \centering
    \begin{tabular}{|l|l|l|}
        \hline
        \textbf{Model} & \textbf{Advantages} & \textbf{Limitations} \\
        \hline
        Custom LM & Fast & Limited vocabulary, less general \\
        GPT-2 Base & General-purpose baseline & Lacks context alignment \\
        Fine-tuned GPT-2 & Balanced, adaptive & Larger size, compute cost \\
        RAG System & Factual grounding, extensibility & Latency, document dependency \\
        \hline
    \end{tabular}
    \caption{Comparison of models and their trade-offs}
\end{table}

% ===================== CONCLUSIONS AND IMPROVEMENT PERSPECTIVES =====================
\section{Conclusions and Improvement Perspectives}

\subsection{Conclusions}

This project successfully demonstrates the effectiveness of a multi-model approach for conversational chatbot systems. The results show that:

\begin{enumerate}
    \item \textbf{Fine-tuning} significantly improves performance (+4.86\% BERTScore)
    \item \textbf{The RAG system} provides factual and accurate responses
    \item \textbf{The custom model} offers an efficient and specialized alternative
    \item \textbf{The multi-model interface} allows flexible use according to needs
\end{enumerate}

\subsection{Improvement Perspectives}

\subsubsection{Technical Improvements}

\begin{enumerate}
    \item \textbf{Model Optimization:}
    \begin{itemize}[label=--]
        \item Increase custom model size
        \item Use advanced attention techniques
        \item Implement knowledge distillation
    \end{itemize}
    
    \item \textbf{RAG System Improvement:}
    \begin{itemize}[label=--]
        \item Integrate metadata in retrieval
        \item Optimize embeddings with newer models
        \item Implement hybrid retrieval (sparse + dense)
    \end{itemize}
    
    \item \textbf{Performance Optimization:}
    \begin{itemize}[label=--]
        \item Parallelize inferences
        \item Optimize GPU memory
        \item Intelligent embedding caching
    \end{itemize}
\end{enumerate}

\subsubsection{Functional Improvements}

\begin{enumerate}
    \item \textbf{User Interface:}
    \begin{itemize}[label=--]
        \item Responsive mobile interface
        \item Multilingual support
        \item Prompt customization
    \end{itemize}
    
    \item \textbf{Advanced Capabilities:}
    \begin{itemize}[label=--]
        \item Multimodal document support
        \item Integration of external knowledge bases
        \item User feedback system
    \end{itemize}
    \newpage
    \item \textbf{Deployment:}
    \begin{itemize}[label=--]
        \item Containerization with Docker
        \item REST API for integration
        \item Advanced monitoring and logging
    \end{itemize}
\end{enumerate}

\subsection{Impact and Applications}

This multi-model system opens the way to numerous applications:

\begin{itemize}[label=--]
    \item \textbf{Education:} Personalized pedagogical assistant
    \item \textbf{Customer Support:} Chatbot with document access
    \item \textbf{Research:} Documentary research assistant
    \item \textbf{Development:} Development aid tool
\end{itemize}

% ===================== ANNEXES =====================
\section{Annexes}
\subsection{Annexe A : Complete Training Logs}
\begin{lstlisting}[language=Python, caption=Training logs excerpt]
Epoch 1/30: Train Loss = 4.3154, Val Loss = 3.7795
Epoch 2/30: Train Loss = 3.6051, Val Loss = 3.5436
Epoch 3/30: Train Loss = 3.3155, Val Loss = 3.4330
...
Epoch 30/30: Train Loss = 2.0905, Val Loss = 3.4347
\end{lstlisting}

\subsection{Annexe B : Technical Configuration}

\begin{table}[H]
    \centering
    \begin{tabular}{|l|l|}
        \hline
        \textbf{Component} & \textbf{Specification} \\
        \hline
        CPU & Intel i5 8350u \\
        GPU & IntelÂ® UHD Graphics 620 / Google Collab Tesla T4\\
        RAM & 16GB DDR4 \\
        Storage & SSD 512GB \\
        OS & Windows 10 \\
        \hline
    \end{tabular}
    \caption{Hardware configuration used}
\end{table}

\subsection{Annexe C : Code Structure}

\begin{lstlisting}[language=Python, caption=Main project structure]
EducoPilot/
ââ best_dailydialog_chatbot.pth # Custom transformer model weights
ââ dailydialog_tokenizer.pkl    # Custom transformer tokenizer
ââ dialogues_test.txt           # Test dialogue dataset
ââ rag.py                       # RAG system implementation
ââ fineTunedGpt2.py             # GPT-2 fine-tuning script
ââ server.py                    # Streamlit web interface
ââ scores.py                    # BERTScore evaluation
ââ training_log.txt             # Training logs (our model)
ââ requirements.txt             # Project dependencies
ââ visualize_bert.py            # BERT visualization utilities
ââ prompt.py                    # Prompt engineering utilities
ââ commands.txt                 # Command reference
ââ gpt2_inference.py            # GPT-2 inference script
ââ LM.py                        # Language model utilities
ââ fine_tuned_gpt2/             # Fine-tuned GPT-2 model directory
ââ venv/                        # Virtual environment
\end{lstlisting}


\subsection{Annexe D : Team Meeting Minutes}

\begin{table}[H]
    \centering
    \begin{tabular}{|l|l|p{8cm}|}
        \hline
        \textbf{Meeting} & \textbf{Date} & \textbf{Key Activities} \\
        \hline
        Meeting 1 & 28/06/2024 & Project planning, technology selection (GPT-2, Streamlit, ChromaDB), task distribution, timeline establishment \\
        \hline
        Meeting 2 & 01/07/2024 & Model pipeline setup, data formatting issues resolution, embedding and evaluation strategy alignment \\
        \hline
        Meeting 3 & 04/07/2024 & GPT-2 fine-tuning launch, training monitoring integration, Streamlit interface core design \\
        \hline
        Meeting 4 & 08/07/2024 & Model-interface connection, input/output flow testing, inference bug resolution, system parameter finalization \\
        \hline
        Meeting 5 & 11/07/2024 & Documentation completion, full system evaluation, deliverable preparation, final presentation rehearsal \\
        \hline
    \end{tabular}
    \caption{Team meeting timeline and activities}
\end{table}



% ===================== BIBLIOGRAPHY =====================
\newpage
\section{Bibliography}

\begin{thebibliography}{99}

\bibitem{dailydialog}
Li, Y., Su, H., Shen, X., Li, W., Cao, Z., \& Niu, S.
\textit{DailyDialog: A Manually Labelled Multi-turn Dialogue Dataset}.

\bibitem{transformer}
Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, L., \& Polosukhin, I.
\textit{Attention Is All You Need}.

\bibitem{bert}
Devlin, J., Chang, M.-W., Lee, K., \& Toutanova, K.
\textit{BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding}.

\bibitem{gpt2}
Radford, A., Wu, J., Child, R., Luan, D., Amodei, D., \& Sutskever, I.
\textit{Language Models are Unsupervised Multitask Learners}.

\bibitem{rag}
Lewis, P., Perez, E., Piktus, A., Petroni, F., Karpukhin, V., Goyal, N., KÃ¼ttler, H., Lewis, M., Yih, W.-t., RocktÃ¤schel, T., Riedel, S., \& Kiela, D.
\textit{Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks}.

\bibitem{bertscore}
Zhang, T., Kishore, V., Wu, F., Weinberger, K. Q., \& Artzi, Y.
\textit{BERTScore: Evaluating Text Generation with BERT}.


\end{thebibliography}

\end{document} 